{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 2024\n",
      "https://www.wsj.com/news/archive/2024/1/1?page=1\n",
      "Page status code: 200\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n",
      "An error occurred: no such table: articles_index\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'article_titles_json/index_2024_1_1_page_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 190\u001b[39m\n\u001b[32m    186\u001b[39m year = \u001b[32m2024\u001b[39m\n\u001b[32m    187\u001b[39m waiting_time = \u001b[32m7\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[43msearching\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwaiting_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 181\u001b[39m, in \u001b[36msearching\u001b[39m\u001b[34m(year, waiting_time)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m day, month, year \u001b[38;5;129;01min\u001b[39;00m dates:\n\u001b[32m    180\u001b[39m     \u001b[38;5;28mprint\u001b[39m(day, month, year)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[43mscrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_elements_from_web\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwaiting_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     time.sleep(waiting_time)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mWebScrap.get_elements_from_web\u001b[39m\u001b[34m(self, year, month, day, waiting_time)\u001b[39m\n\u001b[32m    132\u001b[39m     db.insert_elements(dict_elements)\n\u001b[32m    134\u001b[39m     count_articles += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_to_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m db.exploration(title_url, day, month, year, \u001b[38;5;28mself\u001b[39m.page_number, \u001b[32m1\u001b[39m, count_articles) \u001b[38;5;66;03m#Page explored\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28mself\u001b[39m.total_articles = \u001b[38;5;28mself\u001b[39m.total_articles + count_articles\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mWebScrap._save_to_json\u001b[39m\u001b[34m(self, article_details, year, month, day)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_save_to_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, article_details, year, month, day):\n\u001b[32m     50\u001b[39m     title_json = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33marticle_titles_json/index_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mday\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_page_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.page_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtitle_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     52\u001b[39m         json.dump(article_details, f)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mArticle details saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'article_titles_json/index_2024_1_1_page_1.json'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class ManagementDB():\n",
    "    def __init__(self, db_name='articlesWSJ.db'):\n",
    "        self.name = db_name\n",
    "        self.conn = sqlite3.connect(self.name)\n",
    "        self.c = self.conn.cursor()\n",
    "\n",
    "    def insert_elements(self, elements):\n",
    "        try:\n",
    "            self.c.execute(\"INSERT INTO articles_index (headline, article_time, year, month, day, keyword, link, scraped_at, scanned_status) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "                  (elements[\"headline\"], elements[\"article_time\"], elements[\"year\"], elements[\"month\"], elements[\"day\"], elements[\"keyword\"],\n",
    "                   elements[\"link\"], elements[\"scraped_at\"], elements[\"scanned_status\"]))\n",
    "            self.conn.commit()\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    def exploration(self, link, day, month, year, page_num, values_or_not, count_articles):\n",
    "        try:\n",
    "            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "            self.c.execute(''' INSERT INTO exploration (link, day, month, year, page_num, checked_at, values_or_not, count_articles)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (link, day, month, year, page_num, current_time, values_or_not, count_articles))\n",
    "    \n",
    "            self.conn.commit()\n",
    "        \n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    def closeDB(self):\n",
    "        self.conn.close()\n",
    "\n",
    "class WebScrap:\n",
    "    def __init__(self):\n",
    "        self.page_number = 1\n",
    "        self.total_articles = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.page_number = 1\n",
    "        self.total_articles = 0\n",
    "        \n",
    "    def _save_to_json(self, article_details, year, month, day):\n",
    "        title_json = f\"article_titles_json/index_{year}_{month}_{day}_page_{self.page_number}.json\"\n",
    "        with open(title_json, 'w') as f:\n",
    "            json.dump(article_details, f)\n",
    "        print(f'Article details saved to {title_json}')\n",
    "\n",
    "    def get_elements_from_web(self, year, month, day, waiting_time):\n",
    "\n",
    "        db = ManagementDB()\n",
    "        end_page = False\n",
    "        \n",
    "        while not end_page:\n",
    "            \n",
    "            title_url = f'https://www.wsj.com/news/archive/{year}/{month}/{day}?page={self.page_number}'\n",
    "            print(title_url)\n",
    "    \n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT x.y; Win64; x64; rv:10.0) Gecko/20100101 Firefox/10.0 '}\n",
    "            page = requests.get(title_url, headers=headers)\n",
    "            \n",
    "            article_details = []\n",
    "\n",
    "            # Check if the request was successful\n",
    "            if page.status_code == 200:\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "                print('Page status code:', page.status_code)\n",
    "\n",
    "                # Find the <ol> element with the class 'WSJTheme--list-reset--3pR-r52l'\n",
    "                ol_element = soup.find('ol', class_='WSJTheme--list-reset--3pR-r52l')\n",
    "        \n",
    "                if ol_element:\n",
    "                    # Find all <article> elements within that <ol> element\n",
    "                    article_elements = ol_element.find_all('article')\n",
    "            \n",
    "                    if not article_elements:\n",
    "\n",
    "                        db.exploration(title_url, day, month, year, self.page_number, 0, 0)\n",
    "                        end_page = True\n",
    "                        self.reset()\n",
    "                        \n",
    "                    else:\n",
    "                        count_articles = 0\n",
    "                        # Extract required information from each <article> element\n",
    "                        for article in article_elements:\n",
    "                            headline_span = article.find('span', class_='WSJTheme--headlineText--He1ANr9C')\n",
    "                            a_tag = article.find('a')\n",
    "                            #article_type_span = article.find('span', class_='WSJTheme--articleType--34Gt-vdG')\n",
    "                            timestamp_p = article.find('p', class_='WSJTheme--timestamp--22sfkNDv')\n",
    "\n",
    "                            headline_text = headline_span.text if headline_span else \"N/A\"\n",
    "                            article_link = a_tag['href'] if a_tag else \"N/A\"\n",
    "                            #article_type = article_type_span.text if article_type_span else \"N/A\"\n",
    "                            article_time = timestamp_p.text if timestamp_p else \"N/A\"\n",
    "\n",
    "                            ####Article type####\n",
    "                            article_type_div = article.find('div', class_='WSJTheme--articleType--34Gt-vdG')\n",
    "                            empty_class_span = None\n",
    "\n",
    "                            if article_type_div:\n",
    "                                empty_class_span = article_type_div.find('span', class_='')\n",
    "\n",
    "                            article_type_text = empty_class_span.text if empty_class_span else \"N/A\"\n",
    "                            ####################\n",
    "\n",
    "                            # Adding the current local time of scraping\n",
    "                            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "                            dict_elements = {\n",
    "                                'headline': headline_text,\n",
    "                                'article_time': article_time,\n",
    "                \n",
    "                                'year' : year,\n",
    "                                'month' : month,\n",
    "                                'day' : day,\n",
    "                \n",
    "                                'keyword': article_type_text,\n",
    "                                'link': article_link,\n",
    "                \n",
    "                                'scraped_at': current_time,\n",
    "                                'scanned_status':0,\n",
    "                            }\n",
    "\n",
    "                            article_details.append(dict_elements)\n",
    "\n",
    "                            db.insert_elements(dict_elements)\n",
    "                    \n",
    "                            count_articles += 1\n",
    "                       \n",
    "                        \n",
    "                        self._save_to_json(article_details, year, month, day)\n",
    "\n",
    "                        db.exploration(title_url, day, month, year, self.page_number, 1, count_articles) #Page explored\n",
    "                        self.total_articles = self.total_articles + count_articles\n",
    "                        \n",
    "                        if count_articles == 50:\n",
    "                            self.page_number +=1\n",
    "                            time.sleep(waiting_time)\n",
    "                        else:\n",
    "                            print(f'Articles in the day {self.total_articles}')\n",
    "                            end_page = True\n",
    "                            #db.closeDB()\n",
    "                            self.reset()\n",
    "                        \n",
    "                else:\n",
    "                    print(\"Could not find <ol> element with the specified class.\")\n",
    "                    end_page = True\n",
    "                    self.reset()\n",
    "\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the page. Status code: {page.status_code}\")\n",
    "                end_page = True\n",
    "                self.reset()\n",
    "        \n",
    "        db.closeDB()\n",
    "        \n",
    "def get_dates(year):\n",
    "    start_date = datetime(year, 1, 1)\n",
    "    end_date = datetime(year, 12, 31)\n",
    "    date_list = []\n",
    "    \n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_list.append([current_date.day, current_date.month, current_date.year])\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    return(date_list)\n",
    "    \n",
    "\n",
    "def searching(year, waiting_time):\n",
    "    dates = get_dates(year)\n",
    "    scrap = WebScrap()\n",
    "    for day, month, year in dates:\n",
    "        print(day, month, year)\n",
    "        scrap.get_elements_from_web(year, month, day, waiting_time)\n",
    "        time.sleep(waiting_time) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Time span 2012/01/01-2023/01/01\n",
    "    year = 2024\n",
    "    waiting_time = 7\n",
    "    \n",
    "    \n",
    "    searching(year, waiting_time)\n",
    "    print(\"end\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
