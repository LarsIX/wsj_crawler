{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31f2f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import sys\n",
    "\n",
    "# Add the WSJ directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# import text_cleaner\n",
    "from text_cleaner_WSJ import clean_article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the SQLite database and read the data into a DataFrame\n",
    "db_path = \"articlesWSJ.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "df = pd.read_sql_query(\"SELECT * FROM articles_index\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189b505",
   "metadata": {},
   "source": [
    "Based on the literature review and exploratory analysis (see Expos√©), several article categories were identified as irrelevant to the research objective. These are excluded in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1dc186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract link column  \n",
    "articles_links = df['link']\n",
    "\n",
    "# extract the section names from the links\n",
    "articles_sections = [x[19:40] for x in articles_links]\n",
    "articles_sections = [x.split('/')[1] for x in articles_sections]\n",
    "\n",
    "# add section names to the DataFrame\n",
    "df['section'] = articles_sections\n",
    "\n",
    "# extract unique section names\n",
    "unique_sections = set(articles_sections)\n",
    "print(unique_sections)\n",
    "\n",
    "# create list of irrelevant sections for later removal\n",
    "irrelevant_sections = [\"health\",\"arts-culture\",\"lifestyle\",\"real-estate\",\"sports\",\"livecoverage\",\"personal-finance\",\"video\",\"science\",\"style\",\"articles\"]\n",
    "\n",
    "# investigate headlines\n",
    "print(df[df[\"headline\"].duplicated()])  # Check for duplicates in headlines\n",
    "display(df[df[\"headline\"].duplicated()])  # Display duplicates in headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove irrelevant sections from the DataFrame\n",
    "df_filtered = df[~df['section'].isin(irrelevant_sections)].copy()\n",
    "\n",
    "# remove salt and pepper noise from the headlines as manual investigation showed that they are not relevant for the analysis (only comics)\n",
    "df_filtered = df_filtered[~df_filtered['headline'].str.contains(r'\\b(salt|pepper)\\b', case=False, na=False)]\n",
    "\n",
    "# verify sections value counts after filtering\n",
    "print(df_filtered['section'].value_counts())  # See what's left\n",
    "\n",
    "# verify drop of duplicates in headlines\n",
    "print(df_filtered[df_filtered[\"headline\"].duplicated()])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for misssing values\n",
    "print(df_filtered.isnull().sum())  # Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86667535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path for new DB\n",
    "cleaned_db_path = r\"articles_index_cleaned_2024.db\"\n",
    "\n",
    "# Save DataFrame to a new SQLite database\n",
    "conn = sqlite3.connect(cleaned_db_path)\n",
    "df_filtered.to_sql(\"articles_index_cleaned_2024\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bec426",
   "metadata": {},
   "source": [
    "The article_ids from the cleaned index table are joined with the original articles table (containing the full text) to retain only relevant articles with available corpora. The original database is preserved to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to databases\n",
    "cleaned_index_path = r\"articles_index_cleaned_2024.db\"\n",
    "original_db_path = r\"articlesWSJ_2024.db\"\n",
    "final_db_path = r\"articlesWSJ_clean_final_2024.db\"\n",
    "\n",
    "# Load the cleaned articles_index table\n",
    "print(\"Loading cleaned articles_index from cleaned_index_path...\")\n",
    "conn_cleaned = sqlite3.connect(cleaned_index_path)\n",
    "df_cleaned_index = pd.read_sql_query(\"SELECT * FROM articles_index_cleaned_2024\", conn_cleaned)\n",
    "conn_cleaned.close()\n",
    "print(f\"Loaded {len(df_cleaned_index)} cleaned index entries.\")\n",
    "\n",
    "# Write the cleaned index into the original DB temporarily\n",
    "print(\"Attaching cleaned index to original database...\")\n",
    "conn_full = sqlite3.connect(original_db_path)\n",
    "df_cleaned_index.to_sql(\"articles_index_cleaned_2024\", conn_full, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Perform the join to filter articles based on valid index_ids\n",
    "print(\"Joining article table with cleaned index on index_id...\")\n",
    "query = \"\"\"\n",
    "SELECT article.*, articles_index_cleaned_2024.section, articles_index_cleaned_2024.year, articles_index_cleaned_2024.month, articles_index_cleaned_2024.day\n",
    "FROM article\n",
    "JOIN articlesWSJ_2024\n",
    "ON article.index_id = articlesWSJ_2024.id\n",
    "\"\"\"\n",
    "df_filtered_articles = pd.read_sql_query(query, conn_full)\n",
    "conn_full.close()\n",
    "print(f\"Filtered down to {len(df_filtered_articles)} articles.\")\n",
    "\n",
    "# Save cleaned articles and index into final DB\n",
    "print(\"Saving filtered article and cleaned articles_index into final database...\")\n",
    "conn_final = sqlite3.connect(final_db_path)\n",
    "df_filtered_articles.to_sql(\"article\", conn_final, if_exists=\"replace\", index=False)\n",
    "df_cleaned_index.to_sql(\"articles_index\", conn_final, if_exists=\"replace\", index=False)\n",
    "conn_final.close()\n",
    "\n",
    "print(\"Final cleaned database successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from new database to verify\n",
    "final_db_path = r\"articlesWSJ_clean_final_2024.db\"\n",
    "conn = sqlite3.connect(final_db_path)\n",
    "df = pd.read_sql_query(\"SELECT * FROM article\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicats in corpus\n",
    "duplicates = df[df.duplicated(subset=['corpus'], keep=False)]\n",
    "print(\"Duplicated corpus rows:\\n\", duplicates[['corpus', 'article_id']].head(10))\n",
    "print(\"Number of duplicated corpus rows:\", len(duplicates))\n",
    "\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates(subset=['corpus'], keep='first')   \n",
    "\n",
    "# verify that duplicates are removed\n",
    "duplicates_after = df[df.duplicated(subset=['corpus'], keep=False)] \n",
    "print(\"Duplicated corpus rows after dropping duplicates:\\n\", duplicates_after[['corpus', 'article_id']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a809f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop image_src\n",
    "df = df.drop(columns=['image_src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where article_id is NULL\n",
    "df_no_na = df.dropna(subset=[\"article_id\"])\n",
    "\n",
    "# verify drop\n",
    "print(\"Number of rows after dropping rows with NULL article_id:\", len(df_no_na[df_no_na['article_id'].isnull()]))\n",
    "\n",
    "# check for any duplicates in the 'article_id' column\n",
    "duplicates_article_id = df_no_na[df_no_na.duplicated(subset=['article_id'], keep=False)]\n",
    "print(\"Duplicated article_id rows:\\n\", duplicates_article_id[['article_id', 'corpus']].head(10))\n",
    "\n",
    "# check for duplicates in corpus column\n",
    "duplicates_corpus = df_no_na[df_no_na.duplicated(subset=['corpus'], keep=False)]\n",
    "print(\"Duplicated corpus rows:\\n\", duplicates_corpus[['corpus', 'article_id']].head(10))\n",
    "\n",
    "# check for missing values in the 'corpus' column\n",
    "missing_corpus = df_no_na[(df_no_na['corpus'].isnull()) | (df_no_na['corpus'] == '')]\n",
    "print(\"Missing corpus rows:\\n\", missing_corpus[['article_id', 'corpus']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eed5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merching year, month, day into a single date column\n",
    "df_no_na['date'] = pd.to_datetime(df_no_na[['year', 'month', 'day']].astype(str).agg('-'.join, axis=1), format='%Y-%m-%d')\n",
    "\n",
    "# verify the new date column\n",
    "print(\"Date column:\\n\", df_no_na[['year', 'month', 'day', 'date']].head(10))\n",
    "print(\"Date column data type:\", df_no_na['date'].dtype)\n",
    "\n",
    "# drop the old columns\n",
    "df_no_na = df_no_na.drop(columns=['year', 'month', 'day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cleaner function to clean the corpus\n",
    "cleaned_df = df_no_na.copy()\n",
    "cleaned_df['cleaned_corpus'] = df_no_na['corpus'].apply(clean_article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f49ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify corpus cleaning\n",
    "print(\"Cleaned corpus:\\n\", cleaned_df[['article_id', 'cleaned_corpus']].head(10))\n",
    "print(\"Number of cleaned corpus rows:\", len(cleaned_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overrite the original DataFrame with the cleaned corpus\n",
    "df_no_na['corpus'] = cleaned_df['cleaned_corpus']\n",
    "\n",
    "# drop # drop image_src\n",
    "df_no_na = df_no_na.drop(columns=['image_src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily expand column width and disable truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Inspect full cleaned corpus\n",
    "print(\"Cleaned corpus:\\n\", df_no_na['corpus'].iloc[:10])\n",
    "print(\"column names:\\n\", df_no_na.columns)\n",
    "print(\"DataFrame info:\\n\", df_no_na.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a64fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the final database\n",
    "final_db_path = r\"articlesWSJ_clean_final_2024.db\"\n",
    "conn = sqlite3.connect(final_db_path)\n",
    "\n",
    "# Write the cleaned DataFrame to the 'article' table\n",
    "df_no_na.to_sql('article', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92103e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the corpus is cleaned\n",
    "conn = sqlite3.connect(final_db_path)\n",
    "df = pd.read_sql_query(\"SELECT * FROM article\", conn)\n",
    "conn.close()\n",
    "\n",
    "# column names of the DataFrame \n",
    "print(\"Column names in the DataFrame:\\n\", df.columns)\n",
    "\n",
    "# cleaned corpus\n",
    "print(\"Cleaned corpus:\\n\", df['corpus'].head(10))\n",
    "\n",
    "# check for missing values in the DataFrame\n",
    "print(\"Missing values in the DataFrame:\\n\", df.isnull().sum())\n",
    "\n",
    "# number of rows in the DataFrame\n",
    "print(\"Number of rows in the DataFrame:\", len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
