{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ea96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "# import the annotator function\n",
    "from annotator_function import annotate_articles_with_hype as af\n",
    "\n",
    "# import the mentions ai function\n",
    "from mentions_ai import flag_ai_mentions\n",
    "\n",
    "# import cleaning function\n",
    "from text_cleaner_WSJ import clean_article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71a4e6",
   "metadata": {},
   "source": [
    "Preparing the First Sample\n",
    "\n",
    "The initial sample was drawn from the database articlesWSJ_clean_1.db. As the project progressed, an optimized version — articlesWSJ_clean_final.db — was created to capture articles scraped for days with n < 30 and subsequently apply cleaning (see clean_database_WSJ.ipynb & days_leftover_WSJ.ipynb). For full reproducibility, all database references used during sampling are explicitly included in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce955ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sql file in dataframe\n",
    "final_db_path = r\"C:\\Users\\PC\\Desktop\\Masterarbeit\\Code\\WSJ\\articlesWSJ_clean_1.db\"\n",
    "conn = sqlite3.connect(final_db_path) \n",
    "df = pd.read_sql_query(\"SELECT * FROM article\", conn)\n",
    "\n",
    "# close the connection\n",
    "conn.close() \n",
    "\n",
    "#inspect colums \n",
    "print(df.columns)\n",
    "\n",
    "# insect first 5 rows of the dataframe\n",
    "print(df.head()) \n",
    "\n",
    "# check for duplicates \n",
    "print(f\"There are {df.duplicated().any()} duplicates in the dataframe\")\n",
    "\n",
    "# check for duplicates in article_id\n",
    "print(f\"There are {df['article_id'].duplicated().sum()} duplicates in the article_id column\")\n",
    "\n",
    " # check for null values in corpus\n",
    "print(f\"There are {df['corpus'].isnull().sum()} null values in the corpus column\") \n",
    "\n",
    "# check for empty strings in corpus\n",
    "print(f\"There are {(df['corpus'] == '').sum()} empty strings in the corpus column\")\n",
    "\n",
    "# check number of articles\n",
    "print(f\"There are {df['article_id'].nunique()} unique articles in the dataframe\")\n",
    "\n",
    "# veryfy uniquenes of article_id\n",
    "print(f\"There are {df['article_id'].duplicated().sum()} duplicates in the article_id column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3dee22",
   "metadata": {},
   "source": [
    "Constructing Initial Sample (n = 500)\n",
    "\n",
    "An initial random sample of 500 articles was drawn for manual annotation. Exploratory analysis revealed that the share of AI-related content was too low for effective BERT fine-tuning. As outlined in the exposé, the sample size was later increased to 1,018 articles. From the third batch onward, a revised sampling strategy was applied (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac66f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 510 random article from the dataframe to build annotation data set\n",
    "df_sample_large = df.sample(500, random_state=42)\n",
    "\n",
    "# clean the corpus \n",
    "df_sample_large['corpus'] = df_sample_large['corpus'].apply(lambda x: clean_article_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe02147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the cleaned corpus\n",
    "print(df_sample_large['corpus'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bcf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned sample to a csv file\n",
    "df_sample_large.to_csv(\"articles_WSJ_sub500.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd877e1b",
   "metadata": {},
   "source": [
    "AI-Related Article Filter \n",
    "\n",
    "Uses flag_ai_mentions() to detect AI keywords (AI, A.I., artificial intelligence, machine learning, deep learning, LLM, GPT, ChatGPT, OpenAI, transformer model, generative AI, neural network).  \n",
    "Matching is case-insensitive with word boundaries to avoid false positives.  \n",
    "Ensures each batch contains relevant AI content.  \n",
    "See mentions_ai.py for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag articles with AI mentions by setting mentioned_ai to 1 if the article contains any of the AI-related keywords, 0 otherwise\n",
    "df_sample_large = flag_ai_mentions(df_sample_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc42669",
   "metadata": {},
   "source": [
    "Sample with seed 42 for annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter  annotated articles\n",
    "ai_articles = df_sample_large[df_sample_large['mentions_ai'] == True]\n",
    "non_ai_articles = df_sample_large[df_sample_large['mentions_ai'] == False]\n",
    "\n",
    "# Randomly select 2 AI-related articles and 1 non-AI article (reproducible with seed)\n",
    "sample_ai = ai_articles.sample(4, random_state=42)\n",
    "sample_non_ai = non_ai_articles.sample(1, random_state=42)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df_three_articles = pd.concat([sample_ai, sample_non_ai]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#  Display the result\n",
    "df_three_articles[['article_id', 'title', 'corpus', 'mentions_ai']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64a374",
   "metadata": {},
   "source": [
    "Investigate 5 flagged and an unflagged article to discuss with the annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572abf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter  annotated articles\n",
    "ai_articles = df_sample_large[df_sample_large['mentions_ai'] == True]\n",
    "non_ai_articles = df_sample_large[df_sample_large['mentions_ai'] == False]\n",
    "\n",
    "# Randomly select 3 AI-related articles and 2 non-AI article (reproducible with seed)\n",
    "sample_ai_non_ann = ai_articles.sample(4, random_state=41)\n",
    "sample_non_ai_non_ann = non_ai_articles.sample(1, random_state=41)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df_non_ann = pd.concat([sample_ai_non_ann, sample_non_ai_non_ann]).reset_index(drop=True).drop(columns=['mentions_ai'])\n",
    "\n",
    "# show columns of the dataframe\n",
    "print(df_non_ann.columns)\n",
    "\n",
    "# Display full text for each article in the corpus\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(df_non_ann[['title', 'corpus']])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f09ec7",
   "metadata": {},
   "source": [
    "Constructing the sample for annotation, starting with 100 articles for the first batch. Use AI flags to ensure that at least 50 article mention AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 50 AI-related articles and 50 non-AI article (reproducible with seed)\n",
    "sample_ai_100 = ai_articles.sample(50, random_state=42)\n",
    "sample_non_ai_100 = non_ai_articles.sample(50, random_state=42)\n",
    "df_non_ann_100 = pd.concat([sample_ai_100, sample_non_ai_100]).reset_index(drop=True).drop(columns=['mentions_ai'])\n",
    "\n",
    "#verify the sample size\n",
    "print(f\"Number of AI-related articles in the sample: {len(sample_ai_100)}\")\n",
    "print(f\"Number of non-AI articles in the sample: {len(sample_non_ai_100)}\")\n",
    "print(f\"Total number of articles in the sample: {len(df_non_ann_100)}\")\n",
    "print(f'columns: {df_non_ann_100.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv file  \n",
    "df_non_ann_100.to_csv(\"articles_WSJ_batch_one.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007954a",
   "metadata": {},
   "source": [
    "Annotate sample with 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file\n",
    "df_100_sample = pd.read_csv(\"articles_WSJ_batch_one.csv\")\n",
    "\n",
    "# Display the result\n",
    "display(df_100_sample[['article_id', 'title', 'corpus']][1:2])\n",
    "\n",
    "# verify the sample size\n",
    "print(f'There are {df_100_sample.shape[0]} articles in the dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the articles using the annotator function\n",
    "first_Batch_articles_WSJ_author = af(df=df_100_sample);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the annotation process\n",
    "print(f\"Number of articles in the 100 sample: {len(first_Batch_articles_WSJ_author)}\")\n",
    "print(f\"Number of articles in the annotated 100 sample: {len(first_Batch_articles_WSJ_author)}\")\n",
    "\n",
    "# check for unannotated articles\n",
    "print(f\"Number of articles with AI-related annotation: {first_Batch_articles_WSJ_author['label_ai_related'].notnull().sum()}\")\n",
    "\n",
    "# write the annotated sample to a csv file first_Batch_articles_WSJ_author\n",
    "df_100_annotated.to_csv(\"articles_WSJ_batch_one_author.csv\", index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa9a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean df\n",
    "first_sample = pd.read_csv(\"articles_WSJ_batch_one_author.csv\")\n",
    "\n",
    "# verify the loaded file\n",
    "print(f\"Number of articles in the clean df: {len(first_sample)}\")\n",
    "print(f\"columns: {first_sample.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f3507",
   "metadata": {},
   "source": [
    "In the next part, the second set of 100 sampled articles is created, to which 18 are added which mention AI as filtered by AI-related keywords and phrases (see bellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the large sample from the csv file\n",
    "df_first_subsample = pd.read_csv(\"articles_WSJ_sub500.csv\")\n",
    "\n",
    "# load first batch to exclude it from the large sample\n",
    "first_batch = pd.read_csv(\"articles_WSJ_batch_one_author.csv\")\n",
    "\n",
    "# verify the loaded files\n",
    "print(f\"Number of articles in the clean df: {len(df_first_subsample)}\")\n",
    "print(f\"columns: {df_first_subsample.columns}\")\n",
    "print(f\"Number of articles in the first batch: {len(first_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efcb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert article_id to int64\n",
    "first_batch['article_id'] = first_batch['article_id'].astype('int64')\n",
    "df_first_subsample['article_id'] = df_first_subsample['article_id'].astype('int64')\n",
    "\n",
    "# verify the conversion\n",
    "print(first_batch['article_id'].dtype)\n",
    "print(df_first_subsample['article_id'].dtype)\n",
    "\n",
    "# filter fist_sample for article_id not in df_100_annotated\n",
    "first_sample_not_annotated = df_first_subsample[~df_first_subsample['article_id'].isin(first_batch['article_id'])]\n",
    "\n",
    "# print the number of articles in the filtered dataframe\n",
    "print(f\"Number of articles in the filtered dataframe: {len(first_sample_not_annotated)}\")\n",
    "\n",
    "# print the first 5 rows of the filtered dataframe\n",
    "print(first_sample_not_annotated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dcff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag articles with AI mentions by setting mentioned_ai to 1 if the article contains any of the AI-related keywords, 0 otherwise\n",
    "first_sample_not_annotated = flag_ai_mentions(first_sample_not_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d441f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct second sample\n",
    "second_sample_100 = first_sample_not_annotated[first_sample_not_annotated['mentions_ai'] == 1]\n",
    "second_sample_100 = pd.concat([second_sample_100, first_sample_not_annotated[first_sample_not_annotated['mentions_ai'] == 0].sample(100, random_state=42)])\n",
    "\n",
    "# shuffle the sample, reset the index and drop the old index\n",
    "second_sample_100 = second_sample_100.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# verify sample size   \n",
    "print(f\"Number of AI-related articles in the sample: {len(second_sample_100[second_sample_100['mentions_ai'] == 1])}\")\n",
    "print(f\"Number of non-AI articles in the sample: {len(second_sample_100[second_sample_100['mentions_ai'] == 0])}\")\n",
    "\n",
    "# drop the mentions_ai column\n",
    "second_sample_100 = second_sample_100.drop(columns=['mentions_ai'])\n",
    "\n",
    "# save the second sample to a csv file\n",
    "second_sample_100.to_csv(\"articles_WSJ_batch_two.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19428713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the saved file\n",
    "df_second_sample_100 = pd.read_csv(\"articles_WSJ_batch_two.csv\")\n",
    "\n",
    "# Display the result\n",
    "display(df_second_sample_100[['article_id', 'title', 'corpus']][1:10])\n",
    "\n",
    "# print the number of articles in the second sample\n",
    "print(f\"Number of articles in the second sample: {len(df_second_sample_100)}\")\n",
    "\n",
    "# print the columns of the second sample\n",
    "print(f\"Columns in the second sample: {df_second_sample_100.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf7944",
   "metadata": {},
   "source": [
    "Next, the second batch is independently annotated by the author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc35ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the articles using the annotator function\n",
    "df_second_batch_annotated_author = af(df=df_second_batch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv file\n",
    "df_second_batch_annotated_author.to_csv(\"Carticles_WSJ_batch_two_author.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0884ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veryfy the annotation process\n",
    "print(f\"Number of articles in the second sample: {len(df_second_sample)}\")\n",
    "print(f\"Number of articles in the annotated second sample: {len(df_second_sample_annotated)}\")\n",
    "print(f\"Number of articles with AI-related annotation: {df_second_sample_annotated['label_ai_related'].sum()}\")\n",
    "print(f\"Columns in the annotated second batch: {df_second_sample_annotated.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84202236",
   "metadata": {},
   "source": [
    "Sampling the Third Batch\n",
    "\n",
    "As noted above, the third batch is drawn from the full corpus rather than the initial sub-sample. From this point onward, the optimized database (articlesWSJ_clean_final.db) is used as the data source. Given this, clean_article_text does not have to be applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2eb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from the cleaned database\n",
    "path = \"articlesWSJ_clean_final.db\"\n",
    "conn = sqlite3.connect(path)\n",
    "df = pd.read_sql_query(\"SELECT * FROM article\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the columns of the full dataset\n",
    "print(f\"Columns in the full dataset: {df.columns}\")\n",
    "\n",
    "# check for NA\n",
    "print(f\"Number of NA values in the full dataset: {df.isna().sum().sum()}\")\n",
    "\n",
    "# check for NA in the article_id column\n",
    "print(f\"Number of NA values in the article_id column: {df['article_id'].isna().sum()}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7379dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load batch 1 and 2 to exclude them from the full dataset\n",
    "batch_1 = pd.read_csv(\"articles_WSJ_batch_one.csv\")\n",
    "batch_2 = pd.read_csv(\"articles_WSJ_batch_two.csv\")\n",
    "\n",
    "# check the columns of the batch 1 and batch 2\n",
    "print(f\"Columns in the batch 1: {batch_1.columns}\")\n",
    "print(f\"Columns in the batch 2: {batch_2.columns}\")\n",
    "\n",
    "# concatenate the two batches\n",
    "batch_1_2 = pd.concat([batch_1, batch_2], ignore_index=True)   \n",
    "\n",
    "# verify the concatenation\n",
    "print(f\"Number of articles in batch 1 and 2: {len(batch_1_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the articles that are already annotated\n",
    "df_final = df[~df['article_id'].isin(batch_1_2['article_id'])]\n",
    "\n",
    "# verify the exclusion\n",
    "print(f\"Number of articles in the final dataset: {len(df_final)}\")\n",
    "print(len(df_final) + len(batch_1_2) == len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag ai-related articles in the final dataset\n",
    "flagged_df = flag_ai_mentions(df_final)\n",
    "\n",
    "# verify the flagging process\n",
    "print(f\"Number of articles in the flagged dataset: {len(flagged_df)}\")\n",
    "print(f\"Number of AI-related articles in the flagged dataset: {flagged_df['mentions_ai'].sum()}\")\n",
    "print(f\"Number of non-AI articles in the flagged dataset: {len(flagged_df) - flagged_df['mentions_ai'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811711fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter  annotated articles\n",
    "ai_articles = flagged_df[flagged_df['mentions_ai'] == True]\n",
    "non_ai_articles = flagged_df[flagged_df['mentions_ai'] == False]\n",
    "\n",
    "# Randomly select 50 AI-related articles and 50 non-AI article (reproducible with seed)\n",
    "sample_ai = ai_articles.sample(50, random_state=42)\n",
    "sample_non_ai = non_ai_articles.sample(50, random_state=42)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "sampled_batch_three = pd.concat([sample_ai, sample_non_ai]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#  Display the result\n",
    "sampled_batch_three[['article_id', 'title', 'corpus', 'mentions_ai']]\n",
    "\n",
    "# print number of articles in the batch three\n",
    "print(f\"Number of articles in the batch three: {len(sampled_batch_three)}\")\n",
    "\n",
    "# verify existence of 50 AI-related articles and 50 non-AI articles\n",
    "print(f\"Number of AI-related articles in the batch three: {len(sampled_batch_three[sampled_batch_three['mentions_ai'] == 1])}\")\n",
    "\n",
    "# drop the mentions_ai column\n",
    "sampled_batch_three = sampled_batch_three.drop(columns=['mentions_ai'])\n",
    "\n",
    "# write to csv file\n",
    "sampled_batch_three.to_csv(\"articles_WSJ_batch_three.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the saved file\n",
    "print(sampled_batch_three.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f746a",
   "metadata": {},
   "source": [
    "Next, the third batch is independently annotated by the author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the third sample from the csv file\n",
    "df_third_batch = pd.read_csv(\"articles_WSJ_batch_three.csv\")\n",
    "\n",
    "# verify the loaded file\n",
    "print(f\"Number of articles in the clean df: {len(df_third_batch)}\")\n",
    "print(f\"columns: {df_third_batch.columns}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the articles using the annotator function\n",
    "df_third_batch_annotated_author = af(df=df_third_batch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv file\n",
    "df_third_batch_annotated_author.to_csv(\"articles_WSJ_batch_three_author.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the csv file\n",
    "df_third_batch_annotated = pd.read_csv(\"articles_WSJ_batch_three_author.csv\")\n",
    "# check the columns of the annotated batch three\n",
    "print(f\"Columns in the annotated batch three: {df_third_batch_annotated.columns}\")\n",
    "print(f\"Number of articles in the annotated batch three: {len(df_third_batch_annotated)}\")\n",
    "print(f\"Number of articles with AI-related annotation: {df_third_batch_annotated['label_ai_related'].sum()}\")\n",
    "print(f\"The total hype score is: {df_third_batch_annotated['hype_level'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d173a65",
   "metadata": {},
   "source": [
    "Next, the fourth (final) batch (n=700) is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude  batch 1, batch 2 and batch 3 from the full dataset\n",
    "\n",
    "# read the batch 1, 2 & 3 from the csv file\n",
    "batch_1 = pd.read_csv(\"articles_WSJ_batch_one.csv\")\n",
    "batch_2 = pd.read_csv(\"articles_WSJ_batch_two.csv\")\n",
    "batch_3 = pd.read_csv(\"articles_WSJ_batch_three.csv\")\n",
    "\n",
    "# find columns that are not in all three batches\n",
    "cols1 = set(batch_1.columns)\n",
    "cols2 = set(batch_2.columns)\n",
    "cols3 = set(batch_3.columns)\n",
    "\n",
    "print(\"In batch_1 but not in batch_2:\", cols1 - cols2)\n",
    "print(\"In batch_2 but not in batch_1:\", cols2 - cols1)\n",
    "\n",
    "print(\"In batch_1 but not in batch_3:\", cols1 - cols3)\n",
    "print(\"In batch_3 but not in batch_1:\", cols3 - cols1)\n",
    "\n",
    "print(\"In batch_2 but not in batch_3:\", cols2 - cols3)\n",
    "print(\"In batch_3 but not in batch_2:\", cols3 - cols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f509b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to optimization of the cleaning process (see above), some modifacations have to be made to allow merging of the three batches\n",
    "\n",
    "# drop colukns to ensure that the columns of batch 1 and 2 are identical to batch 3\n",
    "batch_1 = batch_1.drop(columns=[\"date\",\"section\"])\n",
    "batch_3 = batch_3.drop(columns=[\"date\",\"section\",\"index_id\",\"scanned_time\",\"image_src\",\"date\"])\n",
    "\n",
    "# rename the columns to match batch 1 and 2\n",
    "batch_2 = batch_2.rename(columns={\"cleaned_corpus\": \"corpus\",})\n",
    "\n",
    "# verify identity of the columns\n",
    "print(f'columns are identical: {set(batch_1.columns) == set(batch_2.columns) == set(batch_3.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two batches\n",
    "batch_1_2_3 = pd.concat([batch_1, batch_2, batch_3], ignore_index=True)   \n",
    "\n",
    "# verify the concatenation\n",
    "print(f'there are 318 articles in the concatenated df:', len(batch_1_2_3) == 318)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from the cleaned database\n",
    "path = \"articlesWSJ_clean_final.db\"\n",
    "conn = sqlite3.connect(path)\n",
    "df = pd.read_sql_query(\"SELECT * FROM article\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5178882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the columns of the full dataset\n",
    "print(f\"Columns in the full dataset: {df.columns}\")\n",
    "print(f\"Number of articles in the full dataset: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for article_id not in batch 1, 2 and 3\n",
    "batch_1_2_3['article_id'] = batch_1_2_3['article_id'].astype('int64')\n",
    "df_final = df[~df['article_id'].isin(batch_1_2_3['article_id'])]\n",
    "\n",
    "# verify the exclusion\n",
    "print(f\"Number of articles in the final dataset: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag ai-related articles in the final dataset\n",
    "flagged_df = flag_ai_mentions(df_final)\n",
    "\n",
    "# verify the flagging process\n",
    "print(f\"Number of articles in the flagged dataset: {len(flagged_df)}\")\n",
    "print(f\"Number of AI-related articles in the flagged dataset: {flagged_df['mentions_ai'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 350 AI-related articles and 350 non-AI article (reproducible with seed)\n",
    "sample_ai = flagged_df[flagged_df['mentions_ai'] == True].sample(350, random_state=42)\n",
    "sample_non_ai = flagged_df[flagged_df['mentions_ai'] == False].sample(350, random_state=42)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "sampled_batch_four = pd.concat([sample_ai, sample_non_ai]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#  Display the result\n",
    "print(f\"Number of articles in the batch four: {len(sampled_batch_four)}\")\n",
    "print(f\"Number of AI-related articles in the batch four: {len(sampled_batch_four[sampled_batch_four['mentions_ai'] == 1])}\")\n",
    "\n",
    "# drop the mentions_ai column\n",
    "sampled_batch_four = sampled_batch_four.drop(columns=['mentions_ai'])\n",
    "\n",
    "# write to csv file\n",
    "sampled_batch_four.to_csv(\"articles_WSJ_batch_four.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2008b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the saved file\n",
    "df_fourth_batch = pd.read_csv(\"\\articles_WSJ_batch_four.csv\")\n",
    "\n",
    "# verify the loaded file\n",
    "print(f\"Number of articles in the clean df: {len(df_fourth_batch)}\")\n",
    "print(f\"columns: {df_fourth_batch.columns}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
